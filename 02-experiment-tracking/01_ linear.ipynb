{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2bd82d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:47:47.485209Z",
     "start_time": "2023-07-03T15:47:47.485039Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f38ec",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41062d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:47:47.494696Z",
     "start_time": "2023-07-03T15:47:47.488045Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries for data preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4add538c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T23:28:33.691937Z",
     "end_time": "2023-07-10T23:28:34.458786Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c34e8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:47:47.501013Z",
     "start_time": "2023-07-03T15:47:47.499506Z"
    }
   },
   "outputs": [],
   "source": [
    "# mlflow.delete_experiment(0)\n",
    "#\n",
    "# mlflow.start_run()\n",
    "# run = mlflow.active_run()\n",
    "# print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "# mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332438db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:56:59.843751Z",
     "start_time": "2023-07-03T15:56:59.837913Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1464985f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:15.813917Z",
     "start_time": "2023-06-12T13:17:15.766267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/Users/amberm/PycharmProjects/MLops/MLops/02-experiment-tracking/mlruns/1', creation_time=1686581306153, experiment_id='1', last_update_time=1686581306153, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sqlite is used as mlflow backend and set the experiment name for tracking\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d226e07",
   "metadata": {},
   "source": [
    "### Train using Linear Regressor and set the experiment to mlflow manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e6479e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:15.819233Z",
     "start_time": "2023-06-12T13:17:15.816843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing Data\n",
    "# function to read the data and remove outliers\n",
    "\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8029eba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:17.502441Z",
     "start_time": "2023-06-12T13:17:15.828019Z"
    }
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# call read and preprocessing functions for train, validation dataset\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_train \u001B[38;5;241m=\u001B[39m \u001B[43mread_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/green_tripdata_2021-01.parquet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m df_val \u001B[38;5;241m=\u001B[39m read_dataframe(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/green_tripdata_2021-02.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m, in \u001B[0;36mread_dataframe\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_dataframe\u001B[39m(filename):\n\u001B[0;32m----> 5\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     df\u001B[38;5;241m.\u001B[39mlpep_dropoff_datetime \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df\u001B[38;5;241m.\u001B[39mlpep_dropoff_datetime)\n\u001B[1;32m      8\u001B[0m     df\u001B[38;5;241m.\u001B[39mlpep_pickup_datetime \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df\u001B[38;5;241m.\u001B[39mlpep_pickup_datetime)\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pandas/io/parquet.py:509\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001B[0m\n\u001B[1;32m    506\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    507\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[0;32m--> 509\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_backend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pandas/io/parquet.py:227\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m path_or_handle, handles, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[1;32m    221\u001B[0m     path,\n\u001B[1;32m    222\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilesystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    223\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    224\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    225\u001B[0m )\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 227\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m     result \u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mto_pandas(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mto_pandas_kwargs)\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pyarrow/parquet/core.py:2926\u001B[0m, in \u001B[0;36mread_table\u001B[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001B[0m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2920\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m keyword is no longer supported with the new \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2921\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasets-based implementation. Specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2922\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muse_legacy_dataset=True\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to temporarily recover the old \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2923\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbehaviour.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2924\u001B[0m     )\n\u001B[1;32m   2925\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2926\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43m_ParquetDatasetV2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2927\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2928\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2929\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2930\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2931\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2932\u001B[0m \u001B[43m        \u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread_dictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2933\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2934\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2935\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2936\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2937\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2938\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2939\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2940\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2941\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m   2942\u001B[0m     \u001B[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001B[39;00m\n\u001B[1;32m   2943\u001B[0m     \u001B[38;5;66;03m# module is not available\u001B[39;00m\n\u001B[1;32m   2944\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m filters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pyarrow/parquet/core.py:2466\u001B[0m, in \u001B[0;36m_ParquetDatasetV2.__init__\u001B[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001B[0m\n\u001B[1;32m   2462\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m single_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2463\u001B[0m     fragment \u001B[38;5;241m=\u001B[39m parquet_format\u001B[38;5;241m.\u001B[39mmake_fragment(single_file, filesystem)\n\u001B[1;32m   2465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mFileSystemDataset(\n\u001B[0;32m-> 2466\u001B[0m         [fragment], schema\u001B[38;5;241m=\u001B[39mschema \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mfragment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphysical_schema\u001B[49m,\n\u001B[1;32m   2467\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[1;32m   2468\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfragment\u001B[38;5;241m.\u001B[39mfilesystem\n\u001B[1;32m   2469\u001B[0m     )\n\u001B[1;32m   2470\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   2472\u001B[0m \u001B[38;5;66;03m# check partitioning to enable dictionary encoding\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pyarrow/_dataset.pyx:1004\u001B[0m, in \u001B[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pyarrow/error.pxi:144\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/exp-tracking-env/lib/python3.9/site-packages/pyarrow/error.pxi:100\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Could not open Parquet input source '<Buffer>': Parquet file size is 0 bytes"
     ]
    }
   ],
   "source": [
    "# call read and preprocessing functions for train, validation dataset\n",
    "df_train = read_dataframe('./data/green_tripdata_2021-01.parquet')\n",
    "df_val = read_dataframe('./data/green_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2f0eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:17.509859Z",
     "start_time": "2023-06-12T13:17:17.509702Z"
    }
   },
   "outputs": [],
   "source": [
    "# info of dataset\n",
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d51ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:17.569634Z",
     "start_time": "2023-06-12T13:17:17.509954Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbfc25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:18.073199Z",
     "start_time": "2023-06-12T13:17:17.552342Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit the dictvectorizer and preprocess data\n",
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e2394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:18.156757Z",
     "start_time": "2023-06-12T13:17:18.073417Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Train a linear model (without mlflow)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "# mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf6f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:18.159646Z",
     "start_time": "2023-06-12T13:17:18.157431Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec332637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:18.163971Z",
     "start_time": "2023-06-12T13:17:18.162239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train another linear model (Lasso)\n",
    "# lr = Lasso(0.01)\n",
    "# lr.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lr.predict(X_val)\n",
    "\n",
    "# mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4999b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T13:17:21.043090Z",
     "start_time": "2023-06-12T13:17:18.166319Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Train a linear model (with mlflow)\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Amber\")\n",
    "\n",
    "    mlflow.log_param(\"train-data-path\", \"./data/green_tripdata_2021-01.parquet\")\n",
    "    mlflow.log_param(\"valid-data-path\", \"./data/green_tripdata_2021-02.parquet\")\n",
    "\n",
    "    alpha = 0.01\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "\n",
    "    lr = Lasso(alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val)\n",
    "\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_artifact(local_path=\"models/lin_reg.bin\", artifact_path=\"models_pickle\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0848c9d6c7d415ad6c477ff7ff8e98694d1a4aa96d0deee89244642e6b630036"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
